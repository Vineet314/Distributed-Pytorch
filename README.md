## Pytorch Distributed Training

This repository contains a collection of examples and utilities for distributed training using PyTorch. It includes various configurations and setups to help you get started with distributed training in different environments.

First, it begins with training on a single GPU, then it progresses to multi-GPU training.

The goal is to understand how to set up distributed training in PyTorch, including distributed data parallelism (DDP) and Fully Sharded Data Parallel (FSDP).

Since I only have acess to a single node with multiple GPUs, multi node training is not covered in this repository.

Maybe in a future version, I will add multi-node training examples.